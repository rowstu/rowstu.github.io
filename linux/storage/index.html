<!doctype html><html lang=en-us><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><title>Storage notes // rowstu.net</title><link rel=stylesheet href=/assets/shared-theme.css><style>.markdown-content{max-width:900px;margin:0 auto;line-height:1.7}.markdown-content h1{font-size:2.5rem;margin-bottom:1rem;color:var(--accent-teal);border-bottom:2px solid var(--border);padding-bottom:.5rem}.markdown-content h2{font-size:2rem;margin-top:2rem;margin-bottom:1rem;color:var(--accent-orange)}.markdown-content h3{font-size:1.5rem;margin-top:1.5rem;margin-bottom:.75rem;color:var(--text-primary)}.markdown-content p{margin-bottom:1rem}.markdown-content code{background:var(--bg-light);padding:.2rem .4rem;border-radius:3px;font-family:jetbrains mono,monospace;font-size:.9em}.markdown-content pre{background:var(--bg-light);padding:1rem;border-radius:6px;overflow-x:auto;margin-bottom:1rem}.markdown-content pre code{background:0 0;padding:0}.markdown-content a{color:var(--accent-teal);text-decoration:none}.markdown-content a:hover{text-decoration:underline}.markdown-content ul,.markdown-content ol{margin-bottom:1rem;padding-left:2rem}.markdown-content li{margin-bottom:.5rem}.markdown-content blockquote{border-left:4px solid var(--accent-teal);padding-left:1rem;margin:1rem 0;color:var(--text-secondary);font-style:italic}.markdown-content table{width:100%;border-collapse:collapse;margin-bottom:1rem}.markdown-content th,.markdown-content td{border:1px solid var(--border);padding:.75rem;text-align:left}.markdown-content th{background:var(--bg-light);font-weight:600}.breadcrumb{padding:1rem 0;color:var(--text-secondary);font-size:.95rem}.breadcrumb a{color:var(--accent-teal);text-decoration:none}.breadcrumb a:hover{text-decoration:underline}</style></head><body><div class=app-header><h1 class=app-title>Storage notes</h1></div><div class=container><div class=breadcrumb><a href=/>Home</a> / <a href=/browse/>Browse</a>/ <a href=/linuxstorage/>Linuxstorage</a></div><div class="card markdown-content"><h2 id=local-storage>Local Storage</h2><p>Local storage is directly attached to a host (SATA/SAS/NVMe). It offers low latency and high performance but limited sharing unless abstracted.</p><hr><h2 id=software-raid>Software RAID</h2><h3 id=what-it-is>What It Is</h3><p>RAID implemented by the operating system (e.g., <code>mdadm</code> on Linux, Windows Storage Spaces).</p><h3 id=common-raid-levels>Common RAID Levels</h3><table><thead><tr><th>RAID</th><th>Min Disks</th><th>Fault Tolerance</th><th>Performance</th><th>Use Case</th></tr></thead><tbody><tr><td>RAID 0</td><td>2</td><td>None</td><td>Excellent</td><td>Scratch, ephemeral data</td></tr><tr><td>RAID 1</td><td>2</td><td>1 disk</td><td>Good reads</td><td>OS disks</td></tr><tr><td>RAID 5</td><td>3</td><td>1 disk</td><td>Read-heavy</td><td>Legacy arrays</td></tr><tr><td>RAID 6</td><td>4</td><td>2 disks</td><td>Slower writes</td><td>Large SATA arrays</td></tr><tr><td>RAID 10</td><td>4</td><td>1 per mirror</td><td>Excellent</td><td>Databases, VMs</td></tr></tbody></table><blockquote><p><strong>Rule of thumb</strong>: RAID 10 beats RAID 5/6 for rebuild speed and predictability.</p></blockquote><hr><h3 id=setup-linux--mdadm>Setup (Linux – mdadm)</h3><ol><li><p>Identify disks</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>lsblk -o NAME,SIZE,TYPE
</span></span></code></pre></div></li><li><p>Create array</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>mdadm --create /dev/md0 --level<span style=color:#f92672>=</span><span style=color:#ae81ff>10</span> --raid-devices<span style=color:#f92672>=</span><span style=color:#ae81ff>4</span> /dev/sd<span style=color:#f92672>[</span>b-e<span style=color:#f92672>]</span>
</span></span></code></pre></div></li><li><p>Format and mount</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>mkfs.xfs /dev/md0
</span></span><span style=display:flex><span>mount /dev/md0 /mnt/data
</span></span></code></pre></div></li><li><p>Persist configuration</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>mdadm --detail --scan &gt;&gt; /etc/mdadm.conf
</span></span></code></pre></div></li></ol><hr><h3 id=performance-implications>Performance Implications</h3><ul><li>CPU overhead (negligible on modern systems)</li><li>Write penalty on parity RAID (5/6)</li><li>Rebuilds stress disks heavily</li></ul><hr><h3 id=recovery-practices>Recovery Practices</h3><ul><li><p>Replace failed disk</p></li><li><p>Re-add to array:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>mdadm /dev/md0 --add /dev/sdf
</span></span></code></pre></div></li><li><p>Monitor rebuild progress:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>watch cat /proc/mdstat
</span></span></code></pre></div></li></ul><hr><h3 id=best-practices>Best Practices</h3><ul><li>Avoid RAID 5 on large SATA drives</li><li>Monitor SMART <strong>before</strong> failures</li><li>Always have backups—RAID ≠ backup</li></ul><hr><h2 id=hardware-raid>Hardware RAID</h2><h3 id=what-it-is-1>What It Is</h3><p>RAID handled by a dedicated controller (e.g., Dell PERC, HPE Smart Array).</p><hr><h3 id=advantages>Advantages</h3><ul><li>Offloads parity calculations</li><li>Boot support</li><li>Vendor tooling and cache (BBU/FBWC)</li></ul><h3 id=disadvantages>Disadvantages</h3><ul><li>Controller lock-in</li><li>Opaque failure modes</li><li>Expensive cache/batteries</li></ul><hr><h3 id=configuration-steps>Configuration Steps</h3><ol><li>Enter RAID BIOS (Ctrl+R / F8 / vendor-specific)</li><li>Create Virtual Disk</li><li>Choose RAID level and stripe size</li><li>Enable write-back cache (only with battery)</li><li>Install OS on logical volume</li></ol><hr><h3 id=monitoring-practices>Monitoring Practices</h3><ul><li>Vendor tools (<code>perccli</code>, <code>storcli</code>)</li><li>SNMP alerts</li><li>Battery health checks</li><li>Patrol reads enabled</li></ul><hr><h3 id=best-practices-1>Best Practices</h3><ul><li>Document controller firmware versions</li><li>Keep spares available</li><li>Prefer passthrough (HBA mode) for ZFS</li></ul><hr><h2 id=non-raid-solutions>Non-RAID Solutions</h2><h2 id=zfs>ZFS</h2><h3 id=what-it-is-2>What It Is</h3><p>A <strong>volume manager + filesystem</strong> with built-in data integrity.</p><blockquote><p>ZFS replaces RAID, LVM, and filesystem layers with one coherent system.</p></blockquote><hr><h3 id=key-features>Key Features</h3><ul><li>Copy-on-write</li><li>End-to-end checksums</li><li>Snapshots & clones</li><li>Native replication (<code>zfs send</code>)</li><li>ARC/L2ARC caching</li></ul><hr><h3 id=installation-zfs-on-linux>Installation (ZFS on Linux)</h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>apt install zfsutils-linux
</span></span></code></pre></div><hr><h3 id=pool-creation>Pool Creation</h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>zpool create tank mirror /dev/sdb /dev/sdc
</span></span></code></pre></div><p>Or RAIDZ:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>zpool create tank raidz2 /dev/sd<span style=color:#f92672>[</span>b-g<span style=color:#f92672>]</span>
</span></span></code></pre></div><hr><h3 id=dataset-management>Dataset Management</h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>zfs create tank/vmdata
</span></span><span style=display:flex><span>zfs set compression<span style=color:#f92672>=</span>lz4 tank/vmdata
</span></span></code></pre></div><hr><h3 id=snapshots--replication>Snapshots & Replication</h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>zfs snapshot tank/vmdata@daily
</span></span><span style=display:flex><span>zfs send tank/vmdata@daily | ssh backup zfs recv backup/vmdata
</span></span></code></pre></div><hr><h3 id=performance-tuning>Performance Tuning</h3><ul><li>Use <strong>ECC RAM</strong></li><li>1 GB RAM per 1 TB storage (rule of thumb)</li><li>Avoid RAIDZ for random I/O workloads</li><li>Set <code>recordsize=16K</code> for databases</li><li>Use mirrors for VM storage</li></ul><hr><h3 id=data-integrity-best-practices>Data Integrity Best Practices</h3><ul><li><p>Monthly scrubs:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>zpool scrub tank
</span></span></code></pre></div></li><li><p>SMART monitoring</p></li><li><p>Never use hardware RAID underneath ZFS</p></li></ul><hr><h2 id=networked-storage-solutions>Networked Storage Solutions</h2><hr><h2 id=san-storage-area-network>SAN (Storage Area Network)</h2><h3 id=what-it-is-3>What It Is</h3><p>Block-level storage over Fibre Channel or iSCSI.</p><hr><h3 id=architecture-components>Architecture Components</h3><ul><li>Storage array</li><li>Fabric switches</li><li>HBAs or iSCSI NICs</li><li>Multipath I/O</li></ul><hr><h3 id=integration-steps>Integration Steps</h3><ol><li>Present LUN from storage array</li><li>Configure zoning / access control</li><li>Enable multipath (<code>multipathd</code>)</li><li>Format filesystem or use as VM datastore</li></ol><hr><h3 id=performance-monitoring>Performance Monitoring</h3><ul><li>Latency (key metric)</li><li>Queue depth</li><li>Path failures</li><li>IOPS consistency</li></ul><hr><h3 id=backup-strategies>Backup Strategies</h3><ul><li>Array snapshots</li><li>Off-array replication</li><li>Host-level backups for app consistency</li></ul><hr><h3 id=best-practices-2>Best Practices</h3><ul><li>Separate SAN traffic</li><li>Use multipath always</li><li>Test path failure scenarios</li></ul><hr><h2 id=nas-network-attached-storage>NAS (Network Attached Storage)</h2><h3 id=what-it-is-4>What It Is</h3><p>File-level storage over NFS/SMB.</p><hr><h3 id=common-use-cases>Common Use Cases</h3><ul><li>Home directories</li><li>Media storage</li><li>Shared project data</li><li>Backups</li></ul><hr><h3 id=implementation-steps>Implementation Steps</h3><ol><li>Deploy NAS (TrueNAS, NetApp, Synology)</li><li>Create datasets/shares</li><li>Configure permissions (UID/GID)</li><li>Export via NFS/SMB</li></ol><hr><h3 id=best-practices-3>Best Practices</h3><ul><li>Use NFS for Linux/VMs</li><li>SMB for Windows</li><li>Enable snapshots</li><li>Avoid VM disks over SMB</li></ul><hr><h2 id=vsan-virtual-san>vSAN (Virtual SAN)</h2><h3 id=what-it-is-5>What It Is</h3><p>Software-defined storage integrated into VMware clusters.</p><hr><h3 id=configuration-overview>Configuration Overview</h3><ol><li>Minimum 3 nodes</li><li>Cache + capacity disks per node</li><li>Enable vSAN in vSphere</li><li>Define storage policies (FTT, stripe width)</li></ol><hr><h3 id=benefits>Benefits</h3><ul><li>Hyperconverged</li><li>Policy-based storage</li><li>Scales with compute</li></ul><hr><h3 id=scalability-practices>Scalability Practices</h3><ul><li>Uniform hardware</li><li>Fast cache tiers</li><li>Monitor resync traffic</li></ul><hr><h3 id=best-practices-4>Best Practices</h3><ul><li>Dedicated vSAN NICs</li><li>Avoid mixed disk types</li><li>Size for failures (FTT=1 minimum)</li></ul><hr><h2 id=4-iscsi>4. iSCSI</h2><h3 id=what-it-is-6>What It Is</h3><p>Block storage over TCP/IP.</p><hr><h3 id=setup-linux-initiator>Setup (Linux Initiator)</h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>apt install open-iscsi
</span></span><span style=display:flex><span>iscsiadm -m discovery -t sendtargets -p 10.0.0.10
</span></span><span style=display:flex><span>iscsiadm -m node --login
</span></span></code></pre></div><hr><h3 id=performance-optimization>Performance Optimization</h3><ul><li>Jumbo frames (end-to-end)</li><li>Dedicated NICs</li><li>Multipath I/O</li><li>Disable delayed ACKs</li></ul><hr><h3 id=use-cases>Use Cases</h3><ul><li>VM datastores</li><li>Database storage</li><li>Boot-from-SAN</li></ul><hr><h3 id=best-practices-5>Best Practices</h3><ul><li>Never route iSCSI traffic</li><li>Use CHAP authentication</li><li>Monitor latency over throughput</li></ul><hr><h2 id=technology-comparison>Technology Comparison</h2><table><thead><tr><th>Technology</th><th>Performance</th><th>Scalability</th><th>Cost</th><th>Ideal Use Case</th></tr></thead><tbody><tr><td>Software RAID</td><td>Medium–High</td><td>Low</td><td>Low</td><td>Local servers</td></tr><tr><td>Hardware RAID</td><td>High</td><td>Low</td><td>High</td><td>Legacy systems</td></tr><tr><td>ZFS</td><td>High</td><td>Medium</td><td>Medium</td><td>Reliable storage</td></tr><tr><td>NAS</td><td>Medium</td><td>Medium</td><td>Low–Medium</td><td>File sharing</td></tr><tr><td>SAN</td><td>Very High</td><td>High</td><td>High</td><td>Enterprise DBs</td></tr><tr><td>vSAN</td><td>High</td><td>High</td><td>High</td><td>Virtualization</td></tr><tr><td>iSCSI</td><td>Medium–High</td><td>Medium</td><td>Medium</td><td>VM storage</td></tr></tbody></table><hr><h2 id=troubleshooting--maintenance>Troubleshooting & Maintenance</h2><h3 id=common-issues>Common Issues</h3><ul><li>Latency spikes → Check disks, queues, rebuilds</li><li>Inconsistent I/O → Check RAID parity or resyncs</li><li>Silent corruption → Use checksummed FS (ZFS)</li></ul><hr><h3 id=maintenance-checklist>Maintenance Checklist</h3><ul><li>Firmware updates</li><li>SMART checks</li><li>Snapshot pruning</li><li>Capacity trend analysis</li><li>Restore testing</li></ul><hr><h2 id=future-proofing-storage>Future-Proofing Storage</h2><ul><li>Prefer <strong>software-defined storage</strong></li><li>NVMe over Fabrics (NVMe-oF)</li><li>Object storage for scale</li><li>Automation (Ansible/Terraform)</li><li>Observability (IO latency > IOPS)</li></ul><hr><h2 id=thoughts->Thoughts &mldr;</h2><ul><li><strong>RAID is not a backup</strong></li><li><strong>Latency matters more than IOPS</strong></li><li><strong>Mirrors beat parity for real workloads</strong></li><li><strong>ZFS is worth the learning curve</strong></li><li><strong>Simple architectures fail less</strong></li></ul></div></div><script src=/assets/shared-nav.js></script></body></html>